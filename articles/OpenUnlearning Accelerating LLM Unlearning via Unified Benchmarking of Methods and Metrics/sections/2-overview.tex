\section{Overview of LLM Unlearning}
\label{sec:overview}

\ou{} uses a common definition of LLM unlearning, where the goal is to eliminate the influence of ``forget set'' ($\Db_{\text{forget}}$), from an LLM $f_{\text{target}}$ to remove associated model capabilities \citep{liu2024rethinking}. The process pursues two primary goals: (i) \textit{Removal}, ensuring influence caused only by $\Db_{\text{forget}}$ is substantially erased, and (ii) \textit{Retention}, maintaining the LLM's utility on unrelated downstream tasks. The setup usually also involves a retain set disjoint from the forget set, used to aid and assess performance preservation.

Formally, given an original model $f_{\text{target}}$ trained on a dataset containing $\Db_{\text{forget}}$, the unlearning process yields an unlearned model $f_{\text{unlearn}}$. The efficacy of unlearning is typically assessed using evaluation metrics, $M$, which quantify the remaining influence of $\Db_{\text{forget}}$ on $f_{\text{unlearn}}$â€”e.g., by computing $M(f_{\text{unlearn}}, \Db_{\text{forget}})$. Concurrently, utility metrics are used to measure the model's performance on general tasks and data outside of $\Db_{\text{forget}}$, ensuring its overall capabilities are preserved.


\paragraph{Unlearning methods:} Some LLM unlearning approaches are prompting-based, detecting sensitive queries at inference time and deploying obfuscation mechanisms \citep{bhaila-etal-2025-soft, muresanu2024unlearnablealgorithmsincontextlearning, gao2024practical}. But these are not practically scalable as forgetting results accumulate. Of greater interest is the removal of the forget set's influence directly from the weights. The techniques involved include finetuning with one or more of: (1) tailored loss functions \citep{maini2024tofu, fan2024simplicity_simnpo, NPO_zhang2024negative, dong-etal-2025-undial, mekala-etal-2025-alternate}, (2) optimization modifications \citep{jia2024soul, wang2025gru, fan2025towards}, (3) localized parameter updates \citep{li2024wmdp, ding2025unified, gao2024ethos}, and (4) alternative-data based approaches \citep{mekala-etal-2025-alternate, xu2025relearn, choi2024optoutinvestigatingentitylevelunlearning, gu2024meowmemorysupervisedllm, maini2024tofu, jin2024rwku}. 

\paragraph{Benchmarks:} \textit{Fine-grained unlearning} typically focuses on erasing influence of specific training instances from a forget set while preserving performance on related instances not present in the forget set. \tofu{} \citep{maini2024tofu} introduces fine-grained \textit{knowledge} unlearning using QA-style data from 200 fictitious authors. KnowUndo \citep{tian-etal-2024-forget} incorporates copyright and privacy aspects through datasets of books and synthetic author profiles. LUME \citep{ramakrishna2025lume} focuses on unlearning sensitive data from novels, biographies, and real-world figures. PISTOL \citep{qiu2024pistol} builds on TOFU with added structural relationships to study the effect of entity connectivity on knowledge unlearning. \muse{} \citep{shi2025muse} also requires fine-grained unlearning, aiming to remove both knowledge, memorization and privacy influence of news articles and copyrighted books. \textit{Open-ended unlearning} tasks do not target the removal of specific
training data; instead, they aim to erase broader concepts or behaviors without access to a defined forget corpus. \wmdp{} involves a safety-alignment focus, targeting which targets undesired behaviors from hazardous knowledge related to curated datasets  \citep{li2024wmdp}. RWKU \citep{jin2024rwku} and \textit{Who's Harry Potter} (WHP) task \citep{eldan2023s} require forgetting all knowledge related to famous entities. While benchmarks like TOFU, MUSE, PISTOL, LUME, and KnowUndo involve creating task models by injecting new knowledge via finetuning with the forget dataset; WMDP, RWKU and WHP \citep{eldan2023s} operate directly on off-the-shelf LLMs to remove existing influence. 


\paragraph{Unlearning evaluations:} Each benchmark task involves multiple evaluations metrics that judge for unlearning success and for general utility preservation. These range from simple probability judgements in \tofu{}, to MIA-attack based metrics in \muse{}, with dozens of metrics across benchmarks in the literature. Evaluating unlearning success is difficult, with several subsequent works questioning the reliability of benchmark metrics in various aspects \citep{kim2025we, lynch2024eightrobust, wang2024pandoraswhiteboxprecisetraining, doshi2024doesunlearning, zhang2025catastrophic}.


% \vspace{-10pt}
% \paragraph{Re-evaluating methods and evaluations:}
% In addition to the development of benchmarks and methods, a growing body of work has critically re-evaluated whether these methods truly achieve unlearning. \citet{kim2025we} argue that many approaches either degrade the model's representational quality or only modify the classifier layer, leading to deceptively high logit-based evaluation scores. \citet{arditi2024rmu} find that RMU performs mostly shallow modifications. \citet{lynch2024eightrobust} stress-test unlearned models through probing, relearning, and advanced jailbreaking techniques, while \citet{doshi2024doesunlearning} show that simple prompting or rephrasing can increase benchmark accuracy by over ten-fold, exposing lingering knowledge. In parallel, recent work has questioned the reliability of unlearning metrics themselves. \citet{wang2025towardseffective} compare metrics for robustness, and \citet{thaker2025position} highlight how current benchmarks are overly optimistic where benign input modifications can both reveal forgotten information and drastically degrade utility.
% Regardless of these valuable critiques, such insights are rarely integrated into existing benchmarks and are often overlooked. \ou{} aims to unify these efforts, serving as a collaborative platform that grounds unlearning research in more realistic, rigorous, and transparent evaluations.


