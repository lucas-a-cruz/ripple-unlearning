\section{Introduction}
\label{sec:introduction}

LLMs often memorize sensitive, copyrighted or harmful content from their vast training data, raising privacy \citep{carlini2023quantifying}, safety \citep{wei2023jailbroken} and legal \citep{karamolegkou2023copyright, regulation2016regulation, ccpa2021} concerns. Ever increasing costs of pre-training and post-training \citep{grattafiori2024llama, team2023gemini, nvidia2024nemotron} prevent re-training in response to deletion requests \citep{liu2024rethinking}. This has motivated the development of machine \textit{unlearning} techniques that allow for “forgetting” training data via efficient post-training interventions \citep{nguyen2022survey, liu2024rethinking}. The goal of unlearning is to eliminate the undesirable influences from specific training data, while maintaining the overall behavior and performance.


There has been a recent surge in LLM unlearning research, yielding numerous proposed methods on several benchmarks. Modifying model weights to achieve unlearning is of the most interest, with many proposed approaches \citep{NPO_zhang2024negative, wang2025llm, li2024wmdp, fan2024simplicity_simnpo, mekala-etal-2025-alternate, dong-etal-2025-undial, jia2024soul, wang2025gru, fan2025towards}  . Concurrently, several benchmarks have been proposed to evaluate unlearning across a wide range of setups, covering aspects such as synthetic fine-grained unlearning, open-ended unlearning, knowledge, PII, memorization and privacy focused unlearning \citep{maini2024tofu, qiu2024pistol, ramakrishna2025lume, shi2025muse, li2024wmdp, qiu2024pistol, tian-etal-2024-forget, jin2024rwku, eldan2023s}. This volume of LLM unlearning research is marked by a notable fragmentation. Different benchmarks use different evaluations, with no consensus on the best evaluations and considerable criticism of existing evaluations \citep{thaker2025position, scholten2024probabilistic, wang2025towardseffective, zhang2025catastrophic, doshi2024doesunlearning, lynch2024eightrobust}. Evaluating unlearning is a nuanced task involving knowledge, privacy, and utility desiderata, which is arguably as hard as achieving unlearning itself \citep{acr_schwarzschild, lucki2025anadv}.
% Furthermore, unlearning methods are implemented in a benchmark-specific manner \citep{fan2024simplicity_simnpo, mekala-etal-2025-alternate}, resulting in duplicated efforts across codebases and inconsistent implementations, lacking a unified framework. % altpo is implemented only on one benchmark, not truly benchmark specific
Unlearning research currently lacks a unified, standardized framework, with current method implementations often tied to specific setups. This fragmentation limits the ability to rigorously evaluate the efficacy of unlearning methods across diverse settings. We envision LLM unlearning evolving within a shared framework that continuously integrates new and improved methods and evaluations---where unlearning methods iteratively improve on benchmarks, and evaluation metrics themselves improve through meta-evaluation and critical feedback. 
To catalyze this vision, we introduce \ou{}: a unified and extensible benchmark designed to standardize, scale, and accelerate progress in machine unlearning for LLMs. 


\paragraph{A unifying framework.} We introduce \ou{} as a one-stop repository for LLM unlearning, consolidating widely-used benchmarks, unlearning methods, evaluation metrics under different interventions. It is easy to use and extend, enabling the enrichment of benchmarks and a deeper analysis of unlearning algorithms. Through this standardized framework, we foster unified research efforts and expedite the creation of effective unlearning techniques and benchmarks.

\textbf{Evaluating evaluations.}  Our framework moves the field towards a standardization of unlearning evaluations by conducting a meta-evaluation of unlearning metrics. To support this, we introduce a collection of over 450+ open-sourced models with known ground truth states, specifically designed to stress-test these metrics. This pool of models enables us to systematically compare 12 unlearning metrics against a set of desiderata that quantify their faithfulness (accuracy in detecting knowledge) and robustness (vulnerability to interventions). Together with corresponding meta-evaluation procedure, this forms the first benchmark of its kind for assessing and improving unlearning evaluation methods.

\paragraph{Benchmarking unlearning techniques.} 
We compare 8 unlearning methods using a suite of 10 metrics, following \citet{ramakrishna2025semeval}'s ranking procedure. While SimNPO \citep{fan2024simplicity_simnpo} performs the best, we also note limitations with the ranking methodology. We release all the evaluated model checkpoints to encourage further community research into principled LLM unlearning benchmarking.

\ou{} has been open-sourced\footnote{Code \faGithub: 
\href{https://github.com/locuslab/open-unlearning}{\texttt{github.com/locuslab/open-unlearning}}; Models \href{https://huggingface.co/open-unlearning}{\texttt{huggingface.co/open-unlearning}}} under the MIT license. Since its release in March 2025, it has already garnered wide attention in the LLM unlearning community, sitting at 250+ GitHub stars, 20k+ model downloads across 450+ publicly released checkpoints, and popular unlearning benchmarks\footnote{\tofu{} \citep{maini2024tofu}~\faGithub \href{https://github.com/locuslab/tofu}{\texttt{github.com/locuslab/tofu}}; \muse{} \citep{shi2025muse} \faGithub~\href{https://github.com/swj0419/muse_bench}{\texttt{github.com/swj0419/muse\_bench}}} now also point to our repository as the official point of maintenance for their work.
