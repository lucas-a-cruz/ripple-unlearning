@inproceedings{
yang2025exploring,
title={Exploring Criteria of Loss Reweighting to Enhance {LLM} Unlearning},
author={Puning Yang and Qizhou Wang and Zhuo Huang and Tongliang Liu and Chengqi Zhang and Bo Han},
booktitle={Forty-second International Conference on Machine Learning},
year={2025},
url={https://openreview.net/forum?id=mGOugCZlAq}
}

@inproceedings{
wang2025rethinking,
title={Rethinking {LLM} Unlearning Objectives: A Gradient Perspective and Go Beyond},
author={Qizhou Wang and Jin Peng Zhou and Zhanke Zhou and Saebyeol Shin and Bo Han and Kilian Q Weinberger},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025},
url={https://openreview.net/forum?id=huo8MqVH6t}
}

@misc{entesari2025constrainedentropicunlearningprimaldual,
      title={Constrained Entropic Unlearning: A Primal-Dual Framework for Large Language Models}, 
      author={Taha Entesari and Arman Hatami and Rinat Khaziev and Anil Ramakrishna and Mahyar Fazlyab},
      year={2025},
      eprint={2506.05314},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2506.05314}, 
}

@misc{yang2025ceucrossentropyunlearning,
      title={CE-U: Cross Entropy Unlearning}, 
      author={Bo Yang},
      year={2025},
      eprint={2503.01224},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2503.01224}, 
}

@misc{pan2025alinfiklearningapproximatelinearized,
      title={ALinFiK: Learning to Approximate Linearized Future Influence Kernel for Scalable Third-Party LLM Data Valuation}, 
      author={Yanzhou Pan and Huawei Lin and Yide Ran and Jiamin Chen and Xiaodong Yu and Weijie Zhao and Denghui Zhang and Zhaozhuo Xu},
      year={2025},
      eprint={2503.01052},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2503.01052}, 
}

@misc{grosse2023studyinglargelanguagemodel,
      title={Studying Large Language Model Generalization with Influence Functions}, 
      author={Roger Grosse and Juhan Bae and Cem Anil and Nelson Elhage and Alex Tamkin and Amirhossein Tajdini and Benoit Steiner and Dustin Li and Esin Durmus and Ethan Perez and Evan Hubinger and Kamilė Lukošiūtė and Karina Nguyen and Nicholas Joseph and Sam McCandlish and Jared Kaplan and Samuel R. Bowman},
      year={2023},
      eprint={2308.03296},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2308.03296}, 
}

@misc{choe2024dataworthgptllmscale,
      title={What is Your Data Worth to GPT? LLM-Scale Data Valuation with Influence Functions}, 
      author={Sang Keun Choe and Hwijeen Ahn and Juhan Bae and Kewen Zhao and Minsoo Kang and Youngseog Chung and Adithya Pratapa and Willie Neiswanger and Emma Strubell and Teruko Mitamura and Jeff Schneider and Eduard Hovy and Roger Grosse and Eric Xing},
      year={2024},
      eprint={2405.13954},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.13954}, 
}


@misc{qwen2025qwen25technicalreport,
      title={Qwen2.5 Technical Report}, 
      author={Qwen and : and An Yang and Baosong Yang and Beichen Zhang and Binyuan Hui and Bo Zheng and Bowen Yu and Chengyuan Li and Dayiheng Liu and Fei Huang and Haoran Wei and Huan Lin and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Yang and Jiaxi Yang and Jingren Zhou and Junyang Lin and Kai Dang and Keming Lu and Keqin Bao and Kexin Yang and Le Yu and Mei Li and Mingfeng Xue and Pei Zhang and Qin Zhu and Rui Men and Runji Lin and Tianhao Li and Tianyi Tang and Tingyu Xia and Xingzhang Ren and Xuancheng Ren and Yang Fan and Yang Su and Yichang Zhang and Yu Wan and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zihan Qiu},
      year={2025},
      eprint={2412.15115},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.15115}, 
}

@article{cobbe2021gsm8k,
  title={Training Verifiers to Solve Math Word Problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and Hesse, Christopher and Schulman, John},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

@misc{hendrycks2021measuringmassivemultitasklanguage,
      title={Measuring Massive Multitask Language Understanding}, 
      author={Dan Hendrycks and Collin Burns and Steven Basart and Andy Zou and Mantas Mazeika and Dawn Song and Jacob Steinhardt},
      year={2021},
      eprint={2009.03300},
      archivePrefix={arXiv},
      primaryClass={cs.CY},
      url={https://arxiv.org/abs/2009.03300}, 
}

@inproceedings{lin-2004-rouge,
    title = "{ROUGE}: A Package for Automatic Evaluation of Summaries",
    author = "Lin, Chin-Yew",
    booktitle = "Text Summarization Branches Out",
    month = jul,
    year = "2004",
    address = "Barcelona, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W04-1013/",
    pages = "74--81"
}

@inproceedings{
ramakrishna2024llmpirate,
title={{LLM}-{PIRATE}: A benchmark for indirect prompt injection attacks in Large Language Models},
author={Anil Ramakrishna and Jimit Majmudar and Rahul Gupta and Devamanyu Hazarika},
booktitle={The Third Workshop on New Frontiers in Adversarial Machine Learning},
year={2024},
url={https://openreview.net/forum?id=qzEzXnw4ng}
}

@inproceedings{
wei2023jailbroken,
title={Jailbroken: How Does {LLM} Safety Training Fail?},
author={Alexander Wei and Nika Haghtalab and Jacob Steinhardt},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=jA235JGM09}
}

@article{regulation2016regulation,
  title   = {{Regulation (EU) 2016/679 of the European Parliament and of the Council}},
  author  = {European Union},
  journal = {Official Journal of the European Union},
  year    = {2016},
}

@article{ccpa2021,
  title   = {{CCPA} Regulations: Final Regulation Text},
  author  = {{CA} OAG},
  journal = {Office of the Attorney General, California Department of Justice},
  year    = {2021},
}

@article{voigt2017eu,
  title     = {{The EU General Data Protection Regulation (GDPR)}},
  author    = {Voigt, Paul and Von dem Bussche, Axel},
  journal   = {A Practical Guide, 1st Ed., Cham: Springer International Publishing},
  volume    = {10},
  pages     = {3152676},
  year      = {2017},
  publisher = {Springer},
}

@article{team2023gemini,
  title={Gemini: a family of highly capable multimodal models},
  author={Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and Millican, Katie and others},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023}
}

@inproceedings{bhaila-etal-2025-soft,
    title = "Soft Prompting for Unlearning in Large Language Models",
    author = "Bhaila, Karuna  and
      Van, Minh-Hao  and
      Wu, Xintao",
    editor = "Chiruzzo, Luis  and
      Ritter, Alan  and
      Wang, Lu",
    booktitle = "Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = apr,
    year = "2025",
    address = "Albuquerque, New Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.naacl-long.204/",
    pages = "4046--4056",
    ISBN = "979-8-89176-189-6",
    abstract = "The widespread popularity of Large Language Models (LLMs), partly due to their emerging in-context learning ability, has highlighted the importance of ethical and safety considerations for deployment. Motivated by corresponding data protection guidelines, we investigate machine unlearning for LLMs. In contrast to the growing literature on fine-tuning methods to achieve unlearning, we focus on a comparatively lightweight alternative called soft prompting to realize unlearning in LLMs. With losses designed to enforce forgetting as well as utility preservation, our framework Soft Prompting for Unlearning (SPUL) learns prompt tokens that are prepended to a query to induce unlearning of specific training examples at inference time without updating LLM parameters. We conduct a rigorous evaluation of the proposed method, and results indicate that SPUL can significantly improve the trade-off between utility and forgetting for text classification and question-answering. We further validate our method with LLMs of varying parameter sizes to highlight its flexibility and provide detailed insights into the choice of hyperparameters and the influence of the size of unlearning data."
}

@misc{muresanu2024unlearnablealgorithmsincontextlearning,
      title={Unlearnable Algorithms for In-context Learning}, 
      author={Andrei Muresanu and Anvith Thudi and Michael R. Zhang and Nicolas Papernot},
      year={2024},
      eprint={2402.00751},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.00751}, 
}

@misc{pawelczyk2024incontextunlearninglanguagemodels,
      title={In-Context Unlearning: Language Models as Few Shot Unlearners}, 
      author={Martin Pawelczyk and Seth Neel and Himabindu Lakkaraju},
      year={2024},
      eprint={2310.07579},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2310.07579}, 
}

@misc{gu2024meowmemorysupervisedllm,
      title={MEOW: MEMOry Supervised LLM Unlearning Via Inverted Facts}, 
      author={Tianle Gu and Kexin Huang and Ruilin Luo and Yuanqi Yao and Yujiu Yang and Yan Teng and Yingchun Wang},
      year={2024},
      eprint={2409.11844},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2409.11844}, 
}

@misc{choi2024optoutinvestigatingentitylevelunlearning,
      title={Opt-Out: Investigating Entity-Level Unlearning for Large Language Models via Optimal Transport}, 
      author={Minseok Choi and Daniel Rim and Dohyun Lee and Jaegul Choo},
      year={2024},
      eprint={2406.12329},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.12329}, 
}

@article{rezaei2024restor,
  title={RESTOR: Knowledge Recovery through Machine Unlearning},
  author={Rezaei, Keivan and Chandu, Khyathi and Feizi, Soheil and Choi, Yejin and Brahman, Faeze and Ravichander, Abhilasha},
  journal={arXiv preprint arXiv:2411.00204},
  year={2024}
}

@inproceedings{tian-etal-2024-forget,
    title = "To Forget or Not? Towards Practical Knowledge Unlearning for Large Language Models",
    author = "Tian, Bozhong  and
      Liang, Xiaozhuan  and
      Cheng, Siyuan  and
      Liu, Qingbin  and
      Wang, Mengru  and
      Sui, Dianbo  and
      Chen, Xi  and
      Chen, Huajun  and
      Zhang, Ningyu",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-emnlp.82/",
    doi = "10.18653/v1/2024.findings-emnlp.82",
    pages = "1524--1537",
    abstract = "Large Language Models (LLMs) trained on extensive corpora inevitably retain sensitive data, such as personal privacy information and copyrighted material. Recent advancements in knowledge unlearning involve updating LLM parameters to erase specific knowledge. However, current unlearning paradigms are mired in vague forgetting boundaries, often erasing knowledge indiscriminately. In this work, we introduce KnowUnDo, a benchmark containing copyrighted content and user privacy domains to evaluate if the unlearning process inadvertently erases essential knowledge. Our findings indicate that existing unlearning methods often suffer from excessive unlearning. To address this, we propose a simple yet effective method, MemFlex, which utilizes gradient information to precisely target and unlearn sensitive parameters. Experimental results show that MemFlex is superior to existing methods in both precise knowledge unlearning and general knowledge retaining of LLMs."
}

@article{qiu2024pistol,
  title={{PISTOL: Dataset compilation pipeline for structural unlearning of LLMs}},
  author={Qiu, Xinchi and Shen, William F and Chen, Yihong and Cancedda, Nicola and Stenetorp, Pontus and Lane, Nicholas D},
  journal={arXiv preprint arXiv:2406.16810},
  year={2024}
}

@article{xu2025relearn,
  title={Relearn: Unlearning via learning for large language models},
  author={Xu, Haoming and Zhao, Ningyuan and Yang, Liming and Zhao, Sendong and Deng, Shumin and Wang, Mengru and Hooi, Bryan and Oo, Nay and Chen, Huajun and Zhang, Ningyu},
  journal={arXiv preprint arXiv:2502.11190},
  year={2025}
}

@article{gao2024ethos,
  title={Ethos: Rectifying language models in orthogonal parameter space},
  author={Gao, Lei and Niu, Yue and Tang, Tingting and Avestimehr, Salman and Annavaram, Murali},
  journal={arXiv preprint arXiv:2403.08994},
  year={2024}
}

@inproceedings{
ding2025unified,
title={Unified Parameter-Efficient Unlearning for {LLM}s},
author={Chenlu Ding and Jiancan Wu and Yancheng Yuan and Jinda Lu and Kai Zhang and Alex Su and Xiang Wang and Xiangnan He},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025},
url={https://openreview.net/forum?id=zONMuIVCAT}
}

@article{fan2025towards,
  title={Towards LLM Unlearning Resilient to Relearning Attacks: A Sharpness-Aware Minimization Perspective and Beyond},
  author={Fan, Chongyu and Jia, Jinghan and Zhang, Yihua and Ramakrishna, Anil and Hong, Mingyi and Liu, Sijia},
  journal={arXiv preprint arXiv:2502.05374},
  year={2025}
}

@inproceedings{
wang2025llm,
title={{LLM} Unlearning via Loss Adjustment with Only Forget Data},
author={Yaxuan Wang and Jiaheng Wei and Chris Yuhao Liu and Jinlong Pang and Quan Liu and Ankit Shah and Yujia Bao and Yang Liu and Wei Wei},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025},
url={https://openreview.net/forum?id=6ESRicalFE}
}

@inproceedings{feng-etal-2024-fine,
    title = "Fine-grained Pluggable Gradient Ascent for Knowledge Unlearning in Language Models",
    author = "Feng, XiaoHua  and
      Chen, Chaochao  and
      Li, Yuyuan  and
      Lin, Zibin",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.566/",
    doi = "10.18653/v1/2024.emnlp-main.566",
    pages = "10141--10155",
}

@article{halimi2022federated,
  title={Federated unlearning: How to efficiently erase a client in fl?},
  author={Halimi, Anisa and Kadhe, Swanand and Rawat, Ambrish and Baracaldo, Nathalie},
  journal={arXiv preprint arXiv:2207.05521},
  year={2022}
}

@article{ginart2019making,
  title={Making ai forget you: Data deletion in machine learning},
  author={Ginart, Antonio and Guan, Melody and Valiant, Gregory and Zou, James Y},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{mekala-etal-2025-alternate,
    title = "Alternate Preference Optimization for Unlearning Factual Knowledge in Large Language Models",
    author = "Mekala, Anmol  and
      Dorna, Vineeth  and
      Dubey, Shreya  and
      Lalwani, Abhishek  and
      Koleczek, David  and
      Rungta, Mukund  and
      Hasan, Sadid  and
      Lobo, Elita",
    booktitle = "Proceedings of the 31st International Conference on Computational Linguistics",
    month = jan,
    year = "2025",
    address = "Abu Dhabi, UAE",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.coling-main.252/",
    pages = "3732--3752",
}


@article{zhuang2024uoe,
  title={{UOE}: Unlearning One Expert Is Enough For {Mixture-of-experts} {LLMS}},
  author={Zhuang, Haomin and Zhang, Yihua and Guo, Kehan and Jia, Jinghan and Liu, Gaowen and Liu, Sijia and Zhang, Xiangliang},
  journal={arXiv preprint arXiv:2411.18797},
  year={2024}
}

@article{tirumala2022memorization,
  title={Memorization without overfitting: Analyzing the training dynamics of large language models},
  author={Tirumala, Kushal and Markosyan, Aram and Zettlemoyer, Luke and Aghajanyan, Armen},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={38274--38290},
  year={2022}
}

@article{fan2025towardsrelearn,
  title={Towards {LLM} Unlearning Resilient to Relearning Attacks: A Sharpness-Aware Minimization Perspective and Beyond},
  author={Fan, Chongyu and Jia, Jinghan and Zhang, Yihua and Ramakrishna, Anil and Hong, Mingyi and Liu, Sijia},
  journal={arXiv preprint arXiv:2502.05374},
  year={2025}
}

@article{ramakrishna2025lume,
  title={{LUME: LLM} unlearning with multitask evaluations},
  author={Ramakrishna, Anil and Wan, Yixin and Jin, Xiaomeng and Chang, Kai-Wei and Bu, Zhiqi and Vinzamuri, Bhanukiran and Cevher, Volkan and Hong, Mingyi and Gupta, Rahul},
  journal={arXiv preprint arXiv:2502.15097},
  year={2025}
}

@misc{shi2023detecting,
  title={Detecting Pretraining Data from Large Language Models},
  author={Weijia Shi and Anirudh Ajith and Mengzhou Xia and Yangsibo Huang and Daogao Liu and Terra Blevins and Danqi Chen and Luke Zettlemoyer},
  year={2023},
  eprint={2310.16789},
  archivePrefix={arXiv},
  primaryClass={cs.CL}
}

@inproceedings{zhang2025mink,
  title={{Min-K\%++}: Improved Baseline for Pre-Training Data Detection from Large Language Models},
  author={Jingyang Zhang and Jingwei Sun and Eric Yeats and Yang Ouyang and Martin Kuo and Jianyi Zhang and Hao Frank Yang and Hai Li},
  booktitle={The Thirteenth International Conference on Learning Representations},
  year={2025},
  url={https://openreview.net/forum?id=ZGkfoufDaU}
}

@article{
lucki2025anadv,
title={An Adversarial Perspective on Machine Unlearning for {AI} Safety},
author={Jakub {\L}ucki and Boyi Wei and Yangsibo Huang and Peter Henderson and Florian Tram{\`e}r and Javier Rando},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2025},
url={https://openreview.net/forum?id=J5IRyTKZ9s},
note={}
}

@article{belrose2023eliciting,
  title={Eliciting latent predictions from transformers with the tuned lens},
  author={Belrose, Nora and Furman, Zach and Smith, Logan and Halawi, Danny and Ostrovsky, Igor and McKinney, Lev and Biderman, Stella and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2303.08112},
  year={2023}
}

@article{seyitouglu2024extracting,
  title={Extracting Unlearned Information from {LLMs} with Activation Steering},
  author={Seyito{\u{g}}lu, Atakan and Kuvshinov, Aleksei and Schwinn, Leo and G{\"u}nnemann, Stephan},
  journal={arXiv preprint arXiv:2411.02631},
  year={2024}
}

@article{doshi2024doesunlearning,
  title={Does unlearning truly unlearn? {A} black box evaluation of {LLM} unlearning methods},
  author={Doshi, Jai and Stickland, Asa Cooper},
  journal={arXiv preprint arXiv:2411.12103},
  year={2024}
}

@misc{wang2024pandoraswhiteboxprecisetraining,
  title={Pandora's White-Box: Precise Training Data Detection and Extraction in Large Language Models},
  author={Jeffrey G. Wang and Jason Wang and Marvin Li and Seth Neel},
  year={2024},
  eprint={2402.17012},
  archivePrefix={arXiv},
  primaryClass={cs.CR},
  url={https://arxiv.org/abs/2402.17012}
}

@inproceedings{yeom2018privacy,
  title={Privacy risk in machine learning: Analyzing the connection to overfitting},
  author={Yeom, Samuel and Giacomelli, Irene and Fredrikson, Matt and Jha, Somesh},
  booktitle={2018 IEEE 31st computer security foundations symposium (CSF)},
  pages={268--282},
  year={2018},
  organization={IEEE}
}

@inproceedings{hu2025jogging,
  title={Unlearning or Obfuscating? {J}ogging the Memory of Unlearned {LLM}s via Benign Relearning},
  author={Shengyuan Hu and Yiwei Fu and Steven Wu and Virginia Smith},
  booktitle={The Thirteenth International Conference on Learning Representations},
  year={2025},
  url={https://openreview.net/forum?id=fMNRYBvcQN}
}

@article{team2024gemma,
  title={Gemma: {Open} models based on gemini research and technology},
  author={{Gemma Team} and Mesnard, Thomas and Hardin, Cassidy and Dadashi, Robert and Bhupatiraju, Surya and Pathak, Shreya and Sifre, Laurent and Rivi{\`e}re, Morgane and Kale, Mihir Sanjay and Love, Juliette and others},
  journal={arXiv preprint arXiv:2403.08295},
  year={2024}
}

@inproceedings{thaker2025position,
  title={Position: {LLM} Unlearning Benchmarks Are Weak Measures of Progress},
  author={Thaker, Pratiksha and Hu, Shengyuan and Kale, Neil and Maurya, Yash and Wu, Zhiwei Steven and Smith, Virginia},
  booktitle={Proceedings of the 3rd IEEE Conference on Secure and Trustworthy Machine Learning (SaTML)},
  year={2025}
}

@article{gandikota2024erasing,
  title={Erasing conceptual knowledge from language models},
  author={Gandikota, Rohit and Feucht, Sheridan and Marks, Samuel and Bau, David},
  journal={arXiv preprint arXiv:2410.02760},
  year={2024}
}

@article{kim2025we,
  title={Are we truly forgetting? {A} critical re-examination of machine unlearning evaluation protocols},
  author={Kim, Yongwoo and Cha, Sungmin and Kim, Donghyun},
  journal={arXiv preprint arXiv:2503.06991},
  year={2025}
}

@inproceedings{
yuan2025acloser,
title={A Closer Look at Machine Unlearning for Large Language Models},
author={Xiaojian Yuan and Tianyu Pang and Chao Du and Kejiang Chen and Weiming Zhang and Min Lin},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025},
url={https://openreview.net/forum?id=Q1MHvGmhyT}
}

@Misc{Yadan2019Hydra,
  author =       {Omry Yadan},
  title =        {Hydra - {A} framework for elegantly configuring complex applications},
  howpublished = {Github},
  year =         {2019},
  url =          {https://github.com/facebookresearch/hydra}
}


@inproceedings{
wang2025towardseffective,
title={Towards Effective Evaluations and Comparisons for {LLM} Unlearning Methods},
author={Qizhou Wang and Bo Han and Puning Yang and Jianing Zhu and Tongliang Liu and Masashi Sugiyama},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025},
url={https://openreview.net/forum?id=wUtCieKuQU}
}

@article{abdin2024phi,
  title={{Phi}-3 technical report: A highly capable language model locally on your phone},
  author={Abdin, Marah and Aneja, Jyoti and Awadalla, Hany and Awadallah, Ahmed and Awan, Ammar Ahmad and Bach, Nguyen and Bahree, Amit and Bakhtiari, Arash and Bao, Jianmin and Behl, Harkirat and others},
  journal={arXiv preprint arXiv:2404.14219},
  year={2024}
}

@misc{li2023textbooksneediiphi15,
      title={{Textbooks Are All You Need II: Phi-1.5 technical report}}, 
      author={Yuanzhi Li and Sébastien Bubeck and Ronen Eldan and Allie Del Giorno and Suriya Gunasekar and Yin Tat Lee},
      year={2023},
      eprint={2309.05463},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2309.05463}, 
}



@inproceedings{
fan2024simplicity_simnpo,
title={Simplicity Prevails: Rethinking Negative Preference Optimization for {LLM} Unlearning},
author={Chongyu Fan and Jiancheng Liu and Licong Lin and Jinghan Jia and Ruiqi Zhang and Song Mei and Sijia Liu},
booktitle={Neurips Safe Generative AI Workshop 2024},
year={2024},
url={https://openreview.net/forum?id=pVACX02m0p}
}

@Misc{accelerate,
  title={Accelerate: Training and inference at scale made simple, efficient and adaptable.},
  author={Sylvain Gugger and Lysandre Debut and Thomas Wolf and Philipp Schmid and Zachary Mueller and Sourab Mangrulkar and Marc Sun and Benjamin Bossan},
  howpublished={\url{https://github.com/huggingface/accelerate}},
  year={2022}
}

@article{jacobs2023ulysses,
  title={{DeepSpeed} {Ulysses}: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models},
  author={Jacobs, Sam Ade and Tanaka, Masahiro and Zhang, Chengming and Zhang, Minjia and Song, Leon and Rajbhandari, Samyam and He, Yuxiong},
  journal={arXiv preprint arXiv:2309.14509},
  year={2023},
  url={http://arxiv.org/abs/2309.14509}
}

@inproceedings{dong-etal-2025-undial,
    title = "{UNDIAL}: Self-Distillation with Adjusted Logits for Robust Unlearning in Large Language Models",
    author = "Dong, Yijiang River  and
      Lin, Hongzhou  and
      Belkin, Mikhail  and
      Huerta, Ramon  and
      Vuli{\'c}, Ivan",
    booktitle = "Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = apr,
    year = "2025",
    address = "Albuquerque, New Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.naacl-long.444/",
    pages = "8827--8840",
    ISBN = "979-8-89176-189-6",
}

@misc{eval-harness,
  author       = {Gao, Leo and Tow, Jonathan and Abbasi, Baber and Biderman, Stella and Black, Sid and DiPofi, Anthony and Foster, Charles and Golding, Laurence and Hsu, Jeffrey and Le Noac'h, Alain and Li, Haonan and McDonell, Kyle and Muennighoff, Niklas and Ociepa, Chris and Phang, Jason and Reynolds, Laria and Schoelkopf, Hailey and Skowron, Aviya and Sutawika, Lintang and Tang, Eric and Thite, Anish and Wang, Ben and Wang, Kevin and Zou, Andy},
  title        = {{The Language Model Evaluation Harness}},
  month        = 07,
  year         = 2024,
  publisher    = {Zenodo},
  version      = {v0.4.3},
  doi          = {10.5281/zenodo.12608602},
  url          = {https://zenodo.org/records/12608602}
}

@article{wang2025gru,
  title={{GRU}: Mitigating the Trade-off between Unlearning and Retention for Large Language Models},
  author={Wang, Yue and Wang, Qizhou and Liu, Feng and Huang, Wei and Du, Yali and Du, Xiaojiang and Han, Bo},
  journal={arXiv preprint arXiv:2503.09117},
  year={2025}
}

@article{yang2025u,
  title={{CE-U: Cross Entropy} Unlearning},
  author={Yang, Bo},
  journal={arXiv preprint arXiv:2503.01224},
  year={2025}
}

BibTeX Record
@inproceedings{
tunstall2024zephyr,
title={Zephyr: Direct Distillation of {LM} Alignment},
author={Lewis Tunstall and Edward Emanuel Beeching and Nathan Lambert and Nazneen Rajani and Kashif Rasul and Younes Belkada and Shengyi Huang and Leandro Von Werra and Cl{\'e}mentine Fourrier and Nathan Habib and Nathan Sarrazin and Omar Sanseviero and Alexander M Rush and Thomas Wolf},
booktitle={First Conference on Language Modeling},
year={2024},
url={https://openreview.net/forum?id=aKkAwZB6JV}
}

@article{ramakrishna2025semeval,
  title={{SemEval}-2025 {T}ask 4: {U}nlearning sensitive content from large language models},
  author={Ramakrishna, Anil and Wan, Yixin and Jin, Xiaomeng and Chang, Kai-Wei and Bu, Zhiqi and Vinzamuri, Bhanukiran and Cevher, Volkan and Hong, Mingyi and Gupta, Rahul},
  journal={arXiv preprint arXiv:2504.02883},
  year={2025}
}

@inproceedings{
zhang2025catastrophic,
title={Catastrophic Failure of {LLM} Unlearning via Quantization},
author={Zhiwei Zhang and Fali Wang and Xiaomin Li and Zongyu Wu and Xianfeng Tang and Hui Liu and Qi He and Wenpeng Yin and Suhang Wang},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025},
url={https://openreview.net/forum?id=lHSeDYamnz}
}

@inproceedings{dettmers_qlora_bitsandbytes,
 author = {Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {10088--10115},
 publisher = {Curran Associates, Inc.},
 title = {{QLoRA}: Efficient Finetuning of Quantized LLMs},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/1feb87871436031bdc0f2beaa62a049b-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}


@inproceedings{wolf-etal-2020-transformers,
  title={Transformers: State-of-the-Art Natural Language Processing},
  author={Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Rémi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
  month=oct,
  year={2020},
  address={Online},
  publisher={Association for Computational Linguistics},
  url={https://www.aclweb.org/anthology/2020.emnlp-demos.6},
  pages={38--45}
}

@inproceedings{shi2025muse,
  title={{MUSE}: Machine Unlearning Six-Way Evaluation for Language Models},
  author={Weijia Shi and Jaechan Lee and Yangsibo Huang and Sadhika Malladi and Jieyu Zhao and Ari Holtzman and Daogao Liu and Luke Zettlemoyer and Noah A. Smith and Chiyuan Zhang},
  booktitle={The Thirteenth International Conference on Learning Representations},
  year={2025},
  url={https://openreview.net/forum?id=TArmA033BU}
}


@article{chen2024mofo,
  title={{MoFO}: Momentum-Filtered Optimizer for Mitigating Forgetting in {LLM} {Fine-Tuning}},
  author={Chen, Yupeng and Wang, Senmiao and Lin, Zhihang and Qin, Zeyu and Zhang, Yushun and Ding, Tian and Sun, Ruoyu},
  journal={arXiv preprint arXiv:2407.20999},
  year={2024}
}

@inproceedings{doan2021theoretical,
  title={A theoretical analysis of catastrophic forgetting through the {NTK} overlap matrix},
  author={Doan, Thang and Bennani, Mehdi Abbana and Mazoure, Bogdan and Rabusseau, Guillaume and Alquier, Pierre},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={1072--1080},
  year={2021},
  organization={PMLR}
}

@inproceedings{karamolegkou2023copyright,
  title={Copyright Violations and Large Language Models},
  author={Antonia Karamolegkou and Jiaang Li and Li Zhou and Anders S{\o}gaard},
  booktitle={The 2023 Conference on Empirical Methods in Natural Language Processing},
  year={2023},
  url={https://openreview.net/forum?id=YokfK5VOoz}
}

@article{NPO_zhang2024negative,
  title={Negative preference optimization: From catastrophic collapse to effective unlearning},
  author={Zhang, Ruiqi and Lin, Licong and Bai, Yu and Mei, Song},
  journal={First Conference on Language Modelling},
  year={2024},
  url={https://openreview.net/pdf?id=MXLBXjQkmb}
}

@article{thaker2024guardrail,
  title={Guardrail baselines for unlearning in {LLMs}},
  author={Thaker, Pratiksha and Maurya, Yash and Smith, Virginia},
  journal={arXiv preprint arXiv:2403.03329},
  year={2024}
}

@article{liu2024rethinking,
  title={Rethinking Machine Unlearning for Large Language Models},
  author={Liu, Sijia and Yao, Yuanshun and Jia, Jinghan and Casper, Stephen and Baracaldo, Nathalie and Hase, Peter and Xu, Xiaojun and Yao, Yuguang and Li, Hang and Varshney, Kush R and others},
  journal={arXiv preprint arXiv:2402.08787},
  year={2024}
}

@article{deletang2023language,
  title={Language modeling is compression},
  author={Del{\'e}tang, Gr{\'e}goire and Ruoss, Anian and Duquenne, Paul-Ambroise and Catt, Elliot and Genewein, Tim and Mattern, Christopher and Grau-Moya, Jordi and Wenliang, Li Kevin and Aitchison, Matthew and Orseau, Laurent and others},
  journal={arXiv preprint arXiv:2309.10668},
  year={2023}
}

@article{zhang2023right,
  title={Right to be forgotten in the era of large language models: Implications, challenges, and solutions},
  author={Zhang, Dawen and Finckenberg-Broman, Pamela and Hoang, Thong and Pan, Shidong and Xing, Zhenchang and Staples, Mark and Xu, Xiwei},
  journal={arXiv preprint arXiv:2307.03941},
  year={2023}
}

@article{jung2024attack,
  title={Attack and Reset for Unlearning: Exploiting Adversarial Noise toward Machine Unlearning through Parameter Re-initialization},
  author={Jung, Yoonhwa and Cho, Ikhyun and Hsu, Shun-Hsiang and Hockenmaier, Julia},
  journal={arXiv preprint arXiv:2401.08998},
  year={2024}
}

@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}

@inproceedings{graves2021amnesiac,
  title={Amnesiac machine learning},
  author={Graves, Laura and Nagisetty, Vineel and Ganesh, Vijay},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  number={13},
  pages={11516--11524},
  year={2021}
}

@inproceedings{chundawat2023can,
  title={Can bad teaching induce forgetting? unlearning in deep networks using an incompetent teacher},
  author={Chundawat, Vikram S and Tarun, Ayush K and Mandal, Murari and Kankanhalli, Mohan},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  number={6},
  pages={7210--7217},
  year={2023}
}

@misc{neurips-2023-machine-unlearning,
  title={{NeurIPS} 2023 - Machine Unlearning},
  author={Triantafillou, Eleni and Pedregosa, Fabian and Hayes, Jamie and Kairouz, Peter and Guyon, Isabelle and Kurmanji, Meghdad and Dziugaite, Gintare Karolina and Triantafillou, Peter and Zhao, Kairan and Sun Hosoya, Lisheng and Jacques Junior, Julio C. S. and Dumoulin, Vincent and Mitliagkas, Ioannis and Escalera, Sergio and Wan, Jun and Dane, Sohier and Demkin, Maggie and Reade, Walter},
  howpublished={Kaggle},
  note={\url{https://kaggle.com/competitions/neurips-2023-machine-unlearning}},
  year={2023}
}

@inproceedings{zheng-etal-2024-llamafactory,
    title = "{L}lama{F}actory: Unified Efficient Fine-Tuning of 100+ Language Models",
    author = "Zheng, Yaowei  and
      Zhang, Richong  and
      Zhang, Junhao  and
      Ye, Yanhan  and
      Luo, Zheyan",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-demos.38/",
    doi = "10.18653/v1/2024.acl-demos.38",
    pages = "400--410",
}



@inproceedings{li2024fast,
  title={{Fast-NTK}: Parameter-Efficient Unlearning for Large-Scale Models},
  author={Li, Guihong and Hsu, Hsiang and Chen, Chun-Fu and Marculescu, Radu},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={227--234},
  year={2024}
}

@article{ma2022learn,
  title={Learn to forget: Machine unlearning via neuron masking},
  author={Ma, Zhuo and Liu, Yang and Liu, Ximeng and Liu, Jian and Ma, Jianfeng and Ren, Kui},
  journal={IEEE Transactions on Dependable and Secure Computing},
  volume={20},
  number={4},
  pages={3194--3207},
  year={2022},
  publisher={IEEE}
}

@article{kurmanji2024towards,
  title={Towards unbounded machine unlearning},
  author={Kurmanji, Meghdad and Triantafillou, Peter and Hayes, Jamie and Triantafillou, Eleni},
  journal={Advances in neural information processing systems},
  volume={36},
  year={2024}
}

@inproceedings{chiang-lee-2023-large,
  title={Can Large Language Models Be an Alternative to Human Evaluations?},
  author={Chiang, Cheng-Han and Lee, Hung-yi},
  editor={Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
  booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month=jul,
  year={2023},
  address={Toronto, Canada},
  publisher={Association for Computational Linguistics},
  url={https://aclanthology.org/2023.acl-long.870},
  doi={10.18653/v1/2023.acl-long.870},
  pages={15607--15631}
}

@inproceedings{chen2021machine,
  title={When machine unlearning jeopardizes privacy},
  author={Chen, Min and Zhang, Zhikun and Wang, Tianhao and Backes, Michael and Humbert, Mathias and Zhang, Yang},
  booktitle={Proceedings of the 2021 ACM SIGSAC conference on computer and communications security},
  pages={896--911},
  year={2021}
}

@article{mattern2023membership,
  title={Membership inference attacks against language models via neighbourhood comparison},
  author={Mattern, Justus and Mireshghallah, Fatemehsadat and Jin, Zhijing and Sch{\"o}lkopf, Bernhard and Sachan, Mrinmaya and Berg-Kirkpatrick, Taylor},
  journal={arXiv preprint arXiv:2305.18462},
  year={2023}
}

@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{triantafillou2024we,
  title={Are we making progress in unlearning? Findings from the first {NeurIPS} unlearning competition},
  author={Triantafillou, Eleni and Kairouz, Peter and Pedregosa, Fabian and Hayes, Jamie and Kurmanji, Meghdad and Zhao, Kairan and Dumoulin, Vincent and Junior, Julio Jacques and Mitliagkas, Ioannis and Wan, Jun and others},
  journal={arXiv preprint arXiv:2406.09073},
  year={2024}
}

@inproceedings{bourtoule2021machine,
  title={Machine unlearning},
  author={Bourtoule, Lucas and Chandrasekaran, Varun and Choquette-Choo, Christopher A and Jia, Hengrui and Travers, Adelin and Zhang, Baiwu and Lie, David and Papernot, Nicolas},
  booktitle={2021 IEEE Symposium on Security and Privacy (SP)},
  pages={141--159},
  year={2021},
  organization={IEEE}
}

@article{liu2024towards,
  title={Towards safer large language models through machine unlearning},
  author={Liu, Zheyuan and Dou, Guangyao and Tan, Zhaoxuan and Tian, Yijun and Jiang, Meng},
  journal={arXiv preprint arXiv:2402.10058},
  year={2024}
}

@misc{liu2024unlearning,
  title={Machine unlearning in 2024},
  author={Liu, Ken Ziyu},
  journal={Ken Ziyu Liu - Stanford Computer Science},
  year={2024},
  month={May},
  url={https://ai.stanford.edu/~kzliu/blog/unlearning}
}

@article{nguyen2022survey,
  title={A survey of machine unlearning},
  author={Nguyen, Thanh Tam and Huynh, Thanh Trung and Nguyen, Phi Le and Liew, Alan Wee-Chung and Yin, Hongzhi and Nguyen, Quoc Viet Hung},
  journal={arXiv preprint arXiv:2209.02299},
  year={2022}
}

@article{DPO_xu2024dpo,
  title={Is {DPO} superior to {PPO} for {LLM} alignment? A comprehensive study},
  author={Xu, Shusheng and Fu, Wei and Gao, Jiaxuan and Ye, Wenjie and Liu, Weilin and Mei, Zhiyu and Wang, Guangju and Yu, Chao and Wu, Yi},
  journal={arXiv preprint arXiv:2404.10719},
  year={2024}
}

@article{xu2024dpo,
  title={Is {DPO} superior to {PPO} for {LLM} alignment? A comprehensive study},
  author={Xu, Shusheng and Fu, Wei and Gao, Jiaxuan and Ye, Wenjie and Liu, Weilin and Mei, Zhiyu and Wang, Guangju and Yu, Chao and Wu, Yi},
  journal={arXiv preprint arXiv:2404.10719},
  year={2024}
}

@article{DPO_rafailov2024direct,
  title={Direct preference optimization: Your language model is secretly a reward model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{li2023textbooks,
  title={Textbooks are all you need {II}: phi-1.5 technical report},
  author={Li, Yuanzhi and Bubeck, Sébastien and Eldan, Ronen and Del Giorno, Allie and Gunasekar, Suriya and Lee, Yin Tat},
  journal={arXiv preprint arXiv:2309.05463},
  year={2023}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{jia2024soul,
  title={Soul: Unlocking the power of second-order optimization for {LLM} unlearning},
  author={Jia, Jinghan and Zhang, Yihua and Zhang, Yimeng and Liu, Jiancheng and Runwal, Bharat and Diffenderfer, James and Kailkhura, Bhavya and Liu, Sijia},
  journal={arXiv preprint arXiv:2404.18239},
  year={2024}
}

@article{dong2024unmemorization,
  title={Unmemorization in Large Language Models via Self-Distillation and Deliberate Imagination},
  author={Dong, Yijiang River and Lin, Hongzhou and Belkin, Mikhail and Huerta, Ramon and Vulić, Ivan},
  journal={arXiv preprint arXiv:2402.10052},
  year={2024}
}

@article{bhaila2024soft,
  title={Soft Prompting for Unlearning in Large Language Models},
  author={Bhaila, Karuna and Van, Minh-Hao and Wu, Xintao},
  journal={arXiv preprint arXiv:2406.12038},
  year={2024}
}

@article{eldan2023s,
  title={Who's Harry Potter? Approximate Unlearning in {LLMs}},
  author={Eldan, Ronen and Russinovich, Mark},
  journal={arXiv preprint arXiv:2310.02238},
  year={2023}
}

@article{maini2024tofu,
  title={{TOFU}: {A} task of fictitious unlearning for {LLMs}},
  author={Maini, Pratyush and Feng, Zhili and Schwarzschild, Avi and Lipton, Zachary C and Kolter, J Zico},
  journal={First Conference On Language Modeling},
  url={https://openreview.net/pdf?id=B41hNBoWLo},
  year={2024}
}

@article{li2024wmdp,
  title={The {WMDP} benchmark: Measuring and reducing malicious use with unlearning},
  author={Li, Nathaniel and Pan, Alexander and Gopal, Anjali and Yue, Summer and Berrios, Daniel and Gatti, Alice and Li, Justin D and Dombrowski, Ann-Kathrin and Goel, Shashwat and Phan, Long and others},
  journal={arXiv preprint arXiv:2403.03218},
  year={2024}
}

@article{ji2024reversing,
  title={Reversing the Forget-Retain Objectives: An Efficient {LLM} Unlearning Framework from Logit Difference},
  author={Ji, Jiabao and Liu, Yujian and Zhang, Yang and Liu, Gaowen and Kompella, Ramana Rao and Liu, Sijia and Chang, Shiyu},
  journal={arXiv preprint arXiv:2406.08607},
  year={2024}
}

@article{liu2024large,
  title={Large Language Model Unlearning via Embedding-Corrupted Prompts},
  author={Liu, Chris Yuhao and Wang, Yaxuan and Flanigan, Jeffrey and Liu, Yang},
  journal={arXiv preprint arXiv:2406.07933},
  year={2024}
}

@article{gao2024practical,
  title={Practical Unlearning for Large Language Models},
  author={Gao, Chongyang and Wang, Lixu and Weng, Chenkai and Wang, Xiao and Zhu, Qi},
  journal={arXiv preprint arXiv:2407.10223},
  year={2024}
}

@inproceedings{yao2023large,
  title={Large Language Model Unlearning},
  author={Yao, Yuanshun and Xu, Xiaojun and Liu, Yang},
  booktitle={Socially Responsible Language Modelling Research},
  year={2023}
}

@inproceedings{chen2023unlearn,
  title={Unlearn What You Want to Forget: Efficient Unlearning for {LLMs}},
  author={Chen, Jiaao and Yang, Diyi},
  booktitle={The 2023 Conference on Empirical Methods in Natural Language Processing},
  year={2023}
}

@article{lynch2024eightrobust,
  title={Eight methods to evaluate robust unlearning in {LLMs}},
  author={Lynch, Aengus and Guo, Phillip and Ewart, Aidan and Casper, Stephen and Hadfield-Menell, Dylan},
  journal={arXiv preprint arXiv:2402.16835},
  year={2024}
}

@misc{arditi2024rmu,
  title={Unlearning via {RMU} is Mostly Shallow},
  author={Arditi, Andy and Chughtai, Bilal},
  year={2024},
  month=jul,
  note={AI Alignment Forum, informal research note},
  url={https://www.lesswrong.com/posts/6QYpXEscd8GuE7BgW/unlearning-via-rmu-is-mostly-shallow}
}

@misc{barbulescu2024textualsequenceownimproving,
  title={To Each (Textual Sequence) Its Own: Improving Memorized-Data Unlearning in Large Language Models},
  author={George-Octavian Barbulescu and Peter Triantafillou},
  year={2024},
  eprint={2405.03097},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/2405.03097}
}

@inproceedings{acr_schwarzschild,
 author = {Schwarzschild, Avi and Feng, Zhili and Maini, Pratyush and Lipton, Zachary C. and Kolter, J. Zico},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {56244--56267},
 publisher = {Curran Associates, Inc.},
 title = {Rethinking LLM Memorization through the Lens of Adversarial Compression},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/66453d578afae006252d2ea090e151c9-Paper-Conference.pdf},
 volume = {37},
 year = {2024}
}


@misc{huang2024offsetunlearninglargelanguage,
  title={Offset Unlearning for Large Language Models},
  author={James Y. Huang and Wenxuan Zhou and Fei Wang and Fred Morstatter and Sheng Zhang and Hoifung Poon and Muhao Chen},
  year={2024},
  eprint={2404.11045},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2404.11045}
}



@article{nvidia2024nemotron,
  title={Nemotron-4 340{B} technical report},
  author={Team, Nvidia and Adler, Bo and Agarwal, Niket and Aithal, Ashwath and Anh, Dong H and Bhattacharya, Pallab and Brundyn, Annika and Casper, Jared and Catanzaro, Bryan and Clay, Sharon and Cohen, Jonathan and others},
  journal={arXiv preprint arXiv:2406.11704},
  year={2024}
}

@article{grattafiori2024llama,
  title={The {Llama 3} Herd of Models},
  author={Grattafiori, Aaron and Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Vaughan, Alex and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024},
  url={https://arxiv.org/abs/2407.21783}
}

@article{shostack2024boy,
  title={The Boy Who Survived: {Removing Harry Potter from an LLM is harder than reported}},
  author={Shostack, Adam},
  journal={arXiv preprint arXiv:2403.12082},
  year={2024}
}

@article{jin2024rwku,
  title={{RWKU}: {B}enchmarking Real-World Knowledge Unlearning for Large Language Models},
  author={Jin, Zhuoran and Cao, Pengfei and Wang, Chenhao and He, Zhitao and Yuan, Hongbang and Li, Jiachun and Chen, Yubo and Liu, Kang and Zhao, Jun},
  journal={arXiv preprint arXiv:2406.10890},
  year={2024}
}

@article{scholten2024probabilistic,
  title={A Probabilistic Perspective on Unlearning and Alignment for Large Language Models},
  author={Scholten, Yan and G{\"u}nnemann, Stephan and Schwinn, Leo},
  journal={arXiv preprint arXiv:2410.03523},
  year={2024}
}

@inproceedings{
shi2024detecting,
title={Detecting Pretraining Data from Large Language Models},
author={Weijia Shi and Anirudh Ajith and Mengzhou Xia and Yangsibo Huang and Daogao Liu and Terra Blevins and Danqi Chen and Luke Zettlemoyer},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=zWqr3MQuNs}
}

@inproceedings{
carlini2023quantifying,
title={Quantifying Memorization Across Neural Language Models},
author={Nicholas Carlini and Daphne Ippolito and Matthew Jagielski and Katherine Lee and Florian Tramer and Chiyuan Zhang},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=TatRHT_1cK}
}

@inproceedings{
duan2024do,
title={Do Membership Inference Attacks Work on Large Language Models?},
author={Michael Duan and Anshuman Suri and Niloofar Mireshghallah and Sewon Min and Weijia Shi and Luke Zettlemoyer and Yulia Tsvetkov and Yejin Choi and David Evans and Hannaneh Hajishirzi},
booktitle={First Conference on Language Modeling},
year={2024},
url={https://openreview.net/forum?id=av0D19pSkU}
}

@inproceedings{carlini2021extracting,
  title={Extracting training data from large language models},
  author={Carlini, Nicholas and Tramer, Florian and Wallace, Eric and Jagielski, Matthew and Herbert-Voss, Ariel and Lee, Katherine and Roberts, Adam and Brown, Tom and Song, Dawn and Erlingsson, Ulfar and others},
  booktitle={30th USENIX Security Symposium (USENIX Security 21)},
  pages={2633--2650},
  year={2021}
}

