import argparse
import json
import os
import random
import socket
import subprocess
import sys
from datetime import datetime
from pathlib import Path

# Adiciona o diretÃ³rio src do third_party ao path para importar o QADataset
# e outros componentes necessÃ¡rios do open-unlearning.
# Isso torna o script executÃ¡vel de qualquer lugar.
third_party_src_path = Path(__file__).parent.parent.parent / "src"
sys.path.insert(0, str(third_party_src_path))

from datasets import Dataset, DatasetDict


def get_free_port():
    """Encontra e retorna uma porta TCP livre."""
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
        s.bind(('', 0))
        return s.getsockname()[1]


def prepare_dataset(
    input_file: Path,
    output_dir: Path,
    group_id: str,
    num_forget_entities: int,
):
    """
    Carrega os dados do benchmark, filtra por um group_id especÃ­fico,
    divide em conjuntos de 'forget' e 'retain' e salva como um dataset Hugging Face.

    Args:
        input_file: Caminho para o arquivo .jsonl do benchmark.
        output_dir: DiretÃ³rio para salvar o dataset processado.
        group_id: O group_id para isolar no experimento.
        num_forget_entities: O nÃºmero de entidades a serem colocadas no conjunto 'forget'.
    """
    print(f"ğŸ“„ Lendo o arquivo de benchmark: {input_file}")
    probes = []
    with open(input_file, "r", encoding="utf-8") as f:
        for line in f:
            probes.append(json.loads(line))

    # Filtra as probes para o group_id especificado
    group_probes = [p for p in probes if p.get("group_id") == group_id]
    if not group_probes:
        print(f"âŒ Erro: Nenhum dado encontrado para o group_id '{group_id}'. Abortando.")
        sys.exit(1)

    print(f"ğŸ”¬ Encontradas {len(group_probes)} probes para o grupo '{group_id}'.")

    # Identifica entidades Ãºnicas dentro do grupo
    entities = sorted(list({p["entity_id"] for p in group_probes}))
    if len(entities) <= num_forget_entities:
        print(
            f"âŒ Erro: O nÃºmero de entidades para esquecer ({num_forget_entities}) "
            f"deve ser menor que o nÃºmero total de entidades no grupo ({len(entities)})."
        )
        sys.exit(1)

    # Seleciona aleatoriamente as entidades para o conjunto 'forget'
    random.shuffle(entities)
    forget_entities = set(entities[:num_forget_entities])
    retain_entities = set(entities[num_forget_entities:])

    print(f"ğŸ¯ Entidades a serem esquecidas ({len(forget_entities)}): {forget_entities}")
    print(f"ğŸ›¡ï¸ Entidades a serem retidas ({len(retain_entities)}): {retain_entities}")

    # Divide as probes nos conjuntos de forget e retain
    forget_probes = [p for p in group_probes if p["entity_id"] in forget_entities]
    retain_probes = [p for p in group_probes if p["entity_id"] in retain_entities]

    print(f"ğŸ“Š DivisÃ£o final: {len(forget_probes)} probes para 'forget', {len(retain_probes)} probes para 'retain'.")

    # Cria um Ãºnico DatasetDict com splits 'forget' e 'retain'
    dataset_dict = DatasetDict({
        "forget": Dataset.from_list(forget_probes),
        "retain": Dataset.from_list(retain_probes),
    })

    # Salva o DatasetDict em um Ãºnico diretÃ³rio
    output_dir.mkdir(parents=True, exist_ok=True)
    dataset_dict.save_to_disk(str(output_dir))
    print(f"ğŸ’¾ DatasetDict salvo em: {output_dir}")
    
    return output_dir


def create_config_files(
    dataset_path: Path,
    dataset_name: str,
    model: str,
    trainer: str,
    run_name: str,
):
    """
    Cria os arquivos de configuraÃ§Ã£o YAML necessÃ¡rios para o framework open-unlearning.
    """
    print("âœï¸  Criando arquivos de configuraÃ§Ã£o YAML...")

    # Os caminhos sÃ£o relativos Ã  raiz do projeto, onde o script serÃ¡ executado
    project_root = Path.cwd()
    config_root = project_root / "third_party/configs"
    dataset_config_dir = config_root / "data/datasets"
    experiment_config_dir = config_root / "experiment/unlearn/collateral_damage"
    dataset_config_dir.mkdir(exist_ok=True, parents=True)
    experiment_config_dir.mkdir(exist_ok=True, parents=True)

    # O caminho do dataset agora aponta para o diretÃ³rio do DatasetDict
    relative_dataset_path = dataset_path.relative_to(project_root)

    # --- ConfiguraÃ§Ã£o do Dataset 'Forget' ---
    forget_config_path = dataset_config_dir / f"collateral_damage_{dataset_name}_forget.yaml"
    forget_config_content = f"""
# Autogerado por run_unlearning_experiment.py
collateral_damage_{dataset_name}_forget:
  handler: QADataset
  args:
    hf_args:
      path: "{relative_dataset_path.as_posix()}"
      split: "forget"
    question_key: "question"
    answer_key: "answer"
    max_length: 512
"""
    forget_config_path.write_text(forget_config_content)
    print(f"   -> Criado: {forget_config_path}")

    # --- ConfiguraÃ§Ã£o do Dataset 'Retain' ---
    retain_config_path = dataset_config_dir / f"collateral_damage_{dataset_name}_retain.yaml"
    retain_config_content = f"""
# Autogerado por run_unlearning_experiment.py
collateral_damage_{dataset_name}_retain:
  handler: QADataset
  args:
    hf_args:
      path: "{relative_dataset_path.as_posix()}"
      split: "retain"
    question_key: "question"
    answer_key: "answer"
    max_length: 512
"""
    retain_config_path.write_text(retain_config_content)
    print(f"   -> Criado: {retain_config_path}")

    # --- ConfiguraÃ§Ã£o do Experimento ---
    exp_config_path = experiment_config_dir / f"{dataset_name}.yaml"
    model_config_name = model.split("/")[-1]
    
    exp_config_content = f"""
# @package _global_

# Experimento de Desaprendizagem de Dano Colateral: {dataset_name}
# Gerado em: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

defaults:
  - override /model: {model_config_name}
  - override /trainer: {trainer}
  - override /collator: DataCollatorForSupervisedDataset
  - override /eval: null
  - override /data/datasets@data.forget: null
  - override /data/datasets@data.retain: null
  - _self_

# ConfiguraÃ§Ã£o do modelo
model:
  model_args:
    pretrained_model_name_or_path: {model}

# ConfiguraÃ§Ã£o dos dados
data:
  anchor: forget
  forget:
    collateral_damage_{dataset_name}_forget:
      handler: QADataset
      args:
        hf_args:
          path: "{relative_dataset_path.as_posix()}"
          split: "forget"
        question_key: "question"
        answer_key: "answer"
        max_length: 512
  retain:
    collateral_damage_{dataset_name}_retain:
      handler: QADataset
      args:
        hf_args:
          path: "{relative_dataset_path.as_posix()}"
          split: "retain"
        question_key: "question"
        answer_key: "answer"
        max_length: 512

# Nome da tarefa
task_name: {run_name}

# Evaluation configuration (optional)
eval: null
retain_logs_path: null
"""
    exp_config_path.write_text(exp_config_content)
    print(f"   -> Criado: {exp_config_path}")
    print("âœ… Arquivos de configuraÃ§Ã£o criados!")
    return exp_config_path


def run_unlearning(
    experiment_config_path: Path,
    trainer: str,
    model: str,
    run_name: str,
    learning_rate: float,
    num_train_epochs: int,
    per_device_train_batch_size: int,
    gradient_accumulation_steps: int,
    use_accelerate: bool,
):
    """
    Executa o script de treinamento de desaprendizagem.
    """
    print("\n" + "="*80)
    print(f"ğŸš€ Iniciando o processo de desaprendizagem com o trainer '{trainer}'...")
    print("="*80)

    train_script_path = Path("third_party/src/train.py")

    # Garante que third_party/src esteja em PYTHONPATH para o subprocesso
    current_python_path = os.environ.get("PYTHONPATH", "")
    # O Path.resolve() garante que o caminho Ã© absoluto, importante para PYTHONPATH
    new_python_path = str(Path("third_party/src").resolve())
    if current_python_path:
        new_python_path = f"{new_python_path}{os.pathsep}{current_python_path}"

    env = os.environ.copy()
    env["PYTHONPATH"] = new_python_path
    env["CUDA_VISIBLE_DEVICES"] = "0,1"

    # O config_name Ã© relativo ao config_path.
    config_name = experiment_config_path.relative_to(Path.cwd() / "third_party/configs/experiment").as_posix()
    print(f'CONFIG PATH = {config_name}')

    # ConstrÃ³i os argumentos para o script de treinamento
    train_args = [
        str(train_script_path),
        f"--config-name=unlearn.yaml",
        f"experiment={config_name}",
        f"trainer={trainer}",
        f"task_name={run_name}",
        f"model={model.split('/')[-1]}",
        f"trainer.args.learning_rate={learning_rate}",
        f"trainer.args.num_train_epochs={num_train_epochs}",
        f"trainer.args.per_device_train_batch_size={per_device_train_batch_size}",
        f"trainer.args.gradient_accumulation_steps={gradient_accumulation_steps}",
        "trainer.args.save_strategy=epoch",
        "trainer.args.eval_strategy=no",
        "trainer.args.logging_steps=10",
        "trainer.args.ddp_find_unused_parameters=false",
        "trainer.args.gradient_checkpointing=true",
    ]

    if use_accelerate:
        print("ğŸ”§ Configurando para execuÃ§Ã£o Multi-GPU com Accelerate e DeepSpeed.")
        port = get_free_port()
        print(f"   -> Porta Principal (MASTER_PORT): {port}")

        # O executÃ¡vel agora Ã© 'accelerate'. Ele gerencia a execuÃ§Ã£o do script.
        command = [
            "accelerate", "launch",
            "--config_file", "third_party/configs/accelerate/default_config.yaml",
            "--main_process_port", str(port),
            *train_args
        ]
    else:
        print("ğŸ”§ Configurando para execuÃ§Ã£o padrÃ£o (sem Accelerate).")
        command = [sys.executable, *train_args]

    print(f"\nComando de execuÃ§Ã£o:\n{' '.join(command)}\n")

    try:
        process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, encoding='utf-8', bufsize=1, env=env)

        for line in iter(process.stdout.readline, ''):
            print(line, end='')

        process.wait()

        if process.returncode != 0:
            print(f"âŒ O processo de desaprendizagem falhou com o cÃ³digo de saÃ­da {process.returncode}.")
            sys.exit(process.returncode)

        print("\nâœ… Processo de desaprendizagem concluÃ­do com sucesso!")

    except FileNotFoundError:
        print(f"âŒ Erro: O executÃ¡vel '{command[0]}' ou o script de treinamento nÃ£o foi encontrado.")
        print("   Certifique-se de que o ambiente estÃ¡ configurado corretamente e que os caminhos estÃ£o corretos.")
        sys.exit(1)
    except Exception as e:
        print(f"âŒ Uma exceÃ§Ã£o ocorreu durante a execuÃ§Ã£o do desaprendizagem: {e}")
        sys.exit(1)


def save_run_summary(
    args,
    dataset_name: str,
    run_name: str,
    timestamp: str,
    data_run_dir: Path,
    dataset_path: Path,
    exp_config_path: Path,
):
    """
    Salva um resumo da execuÃ§Ã£o do experimento em um arquivo JSON.
    """
    print("\n" + "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    print("Step: Saving Run Summary")
    print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    print("")

    summary_data = {
        "group_id": args.group_id,
        "num_forget": args.num_forget,
        "model": args.model,
        "trainer": args.trainer,
        "dataset_name": dataset_name,
        "run_name": run_name,
        "timestamp": timestamp,
        "hyperparameters": {
            "learning_rate": args.learning_rate,
            "num_train_epochs": args.num_train_epochs,
            "per_device_train_batch_size": args.per_device_train_batch_size,
            "gradient_accumulation_steps": args.gradient_accumulation_steps,
        },
        "paths": {
            "input_file": str(args.input_file),
            "processed_dataset_dir": str(dataset_path),
            "experiment_config_file": str(exp_config_path),
            "model_checkpoint_dir": f"saves/unlearn/{run_name}/",
            "run_summary_file": str(data_run_dir / "run_summary.json"),
        }
    }

    summary_file_path = data_run_dir / "run_summary.json"
    summary_file_path.parent.mkdir(parents=True, exist_ok=True)
    with open(summary_file_path, "w", encoding="utf-8") as f:
        json.dump(summary_data, f, indent=2, ensure_ascii=False)

    print(f"Created: {summary_file_path}")
    print("âœ… Run summary saved!")
    print("")


def main():
    parser = argparse.ArgumentParser(description="Executa um experimento de desaprendizagem de dano colateral.")
    parser.add_argument(
        "--group_id",
        type=str,
        required=True,
        help="O 'group_id' do benchmark para focar no experimento (ex: 'michael_jordan')."
    )
    parser.add_argument(
        "--num_forget",
        type=int,
        required=True,
        help="O nÃºmero de entidades a serem usadas no conjunto 'forget'."
    )
    parser.add_argument(
        "--model",
        type=str,
        default="meta-llama/Llama-3.2-1B-Instruct",
        help="O nome do modelo do Hugging Face a ser usado."
    )
    parser.add_argument(
        "--trainer",
        type=str,
        default="GradAscent",
        help="O mÃ©todo de desaprendizagem (trainer) a ser usado (ex: GradAscent, NPO)."
    )
    parser.add_argument(
        "--accelerate",
        action="store_true",
        help="Se definido, usa o Hugging Face Accelerate para executar o treinamento distribuÃ­do."
    )
    parser.add_argument(
        "--input_file",
        type=Path,
        default="data/datasets/collateral_damage_probes.jsonl",
        help="Caminho para o arquivo .jsonl do benchmark."
    )
    # HiperparÃ¢metros de Treinamento
    parser.add_argument("--learning_rate", type=float, default=1e-5, help="Taxa de aprendizado para o treinamento.")
    parser.add_argument("--num_train_epochs", type=int, default=5, help="NÃºmero de Ã©pocas de treinamento.")
    parser.add_argument("--per_device_train_batch_size", type=int, default=1, help="Batch size por dispositivo.")
    parser.add_argument("--gradient_accumulation_steps", type=int, default=16, help="Passos para acumulaÃ§Ã£o de gradiente.")

    args = parser.parse_args()

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    dataset_name = f"{args.group_id.replace(' ', '_')}_{args.num_forget}"
    run_name = f"{dataset_name}_{timestamp}"
    
    project_root = Path.cwd()
    data_run_dir = project_root / f"data/run/{dataset_name}/{timestamp}"

    print("="*80)
    print("ğŸš€ Iniciando Experimento de Desaprendizagem de Dano Colateral ğŸš€")
    print("="*80)
    print(f"ğŸ†” Grupo:               {args.group_id}")
    print(f"ğŸ”¢ Entidades a Esquecer: {args.num_forget}")
    print(f"ğŸ¤– Modelo:              {args.model}")
    print(f"ğŸ“ Trainer:             {args.trainer}")
    print(f"ğŸš€ Accelerate:          {'Sim' if args.accelerate else 'NÃ£o'}")
    print(f"ğŸ·ï¸ Nome da ExecuÃ§Ã£o:    {run_name}")
    print(f"ğŸ“‚ DiretÃ³rio de Dados:  {data_run_dir}")
    print("="*80)

    # --- Passo 1: Preparar o Dataset ---
    dataset_path = prepare_dataset(
        project_root / args.input_file,
        data_run_dir / "dataset",
        args.group_id,
        args.num_forget,
    )

    # --- Passo 2: Criar Arquivos de ConfiguraÃ§Ã£o ---
    exp_config_path = create_config_files(
        dataset_path,
        dataset_name,
        args.model,
        args.trainer,
        run_name,
    )

    # --- Passo 3: Executar o Desaprendizagem ---
    run_unlearning(
        exp_config_path,
        args.trainer,
        args.model,
        run_name,
        args.learning_rate,
        args.num_train_epochs,
        args.per_device_train_batch_size,
        args.gradient_accumulation_steps,
        args.accelerate,
    )

    # --- Passo 4: Salvar Resumo da ExecuÃ§Ã£o ---
    save_run_summary(
        args,
        dataset_name,
        run_name,
        timestamp,
        data_run_dir,
        dataset_path,
        exp_config_path,
    )

    # --- Resumo Final ---
    model_config_name = args.model.split('/')[-1]
    print("\n" + "="*80)
    print("ğŸ‰ Experimento ConcluÃ­do! ğŸ‰")
    print("================================================================================================")
    print("Resumo dos Artefatos Gerados:")
    print(f"  ğŸ“¦ Dataset Processado: {dataset_path}")
    print(f"  ğŸ§  Checkpoint do Modelo: saves/unlearn/{run_name}/")
    print(f"  ğŸ“‹ Config do Experimento: {exp_config_path}")
    print(f"  ğŸ“„ SumÃ¡rio da ExecuÃ§Ã£o: {data_run_dir / 'run_summary.json'}")
    print("\nPrÃ³ximos Passos Sugeridos:")
    print("  1. Avalie o modelo desaprendido para medir o dano colateral:")
    print(f"     python third_party/src/eval.py \
       experiment=eval/tofu/default \
       model={model_config_name} \
       model.model_args.pretrained_model_name_or_path=saves/unlearn/{run_name} \
       task_name={run_name}_eval")
    print("="*80)


if __name__ == "__main__":
    # Garante que o script seja executado a partir da raiz do projeto.
    # Resolve o caminho absoluto do script e sobe na Ã¡rvore de diretÃ³rios.
    project_root = Path(__file__).resolve().parent.parent.parent.parent
    os.chdir(project_root)
    
    main()
    